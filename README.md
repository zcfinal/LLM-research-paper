# LLM-research-paper
paper list about large language model (LLM)

## LLM training

1. "LIMA: Less Is More for Alignment"
    Arxiv (2023).
    [[paper](http://export.arxiv.org/abs/2305.11206)] <br />
    Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy

## LLM Survey

1. "A Survey of Large Language Models"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2303.18223)] <br />
    Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, Ji-Rong Wen

## LLM Evaluation

1. "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2302.06476)] <br />
    Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang
  
2. "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2302.04023)] <br />
    Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, Pascale Fung

3. "We’re Afraid Language Models Aren’t Modeling Ambiguity"
    Arxiv (2023).
    [[paper](https://arxiv.org/pdf/2304.14399.pdf)] <br />
    Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A. Smith, Yejin Choi

## Efficient Tuning

1. "VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"
    CVPR (2022).
    [[paper](https://arxiv.org/abs/2112.06825)] <br />
    Yi-Lin Sung, Jaemin Cho, Mohit Bansal

2. "Multimodal Few-Shot Learning with Frozen Language Models"
    NIPS (2021).
    [[paper](https://proceedings.neurips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf)] <br />
    Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, Felix Hill
    

3. "Modular and Parameter-Efficient Multimodal Fusion with Prompting"
    Findings of ACL (2022).
    [[paper](https://aclanthology.org/2022.findings-acl.234/)] <br />
    Sheng Liang, Mengjie Zhao, Hinrich Schuetze

4. "Learning to prompt for vision-language models"
    IJCV (2022).
    [[paper](https://arxiv.org/pdf/2109.01134.pdf)] <br />
    Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu 
    
5. "Conditional prompt learning for vision-language models"
    CVPR (2022).
    [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf)] <br />
    Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu
    
6. "Maple: Multi-modal prompt learning"
    Arxiv (2022).
    [[paper](https://arxiv.org/pdf/2210.03117.pdf)] <br />
    Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan
    
7. "AIM: Adapting Image Models for Efficient Video Understanding"
    ICLR (2023).
    [[paper](https://arxiv.org/abs/2302.03024)] <br />
    Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, Mu Li

## LLM with Multimodal 

1. "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning"
    Arxiv (2023).
    [[paper](https://arxiv.org/pdf/2301.05226.pdf)] <br />
    Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, Chuang Gan
    
2. "Multimodal Chain-of-Thought Reasoning in Language Models"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2302.00923)] <br />
    Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola
    
3. "Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering"
    CVPR (2023).
    [[paper](https://arxiv.org/abs/2303.01903)]
    [[code](https://github.com/MILVLG/prophet)] <br />
    Zhenwei Shao, Zhou Yu, Meng Wang, Jun Yu
    
4. "Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners"
    CVPR (2023).
    [[paper](https://arxiv.org/abs/2303.02151)]
    [[code](https://github.com/ZrrSkywalker/CaFo)] <br />
    Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, Peng Gao
    
5. "An empirical study of gpt-3 for few-shot knowledge-based vqa"
    AAAI (2022).
    [[paper](https://arxiv.org/abs/2109.05014)] <br />
    Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, Lijuan Wang
    
6. "KAT: A Knowledge Augmented Transformer for Vision-and-Language"
    NAACL (2022).
    [[paper](https://aclanthology.org/2022.naacl-main.70/)] <br />
    Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, Jianfeng Gao
    
7. "REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering"
    NIPS (2022).
    [[paper](https://arxiv.org/abs/2206.01201)] <br />
    Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chenguang Zhu, Lu Yuan
    
8. "DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training"
    ICLR (2023).
    [[paper](https://arxiv.org/abs/2303.03032)] <br />
    Wei Li, Linchao Zhu, Longyin Wen, Yi Yang

9. "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2303.04671)] <br />
    Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan
    
10. "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2303.11381)] <br />
    Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang

11. "ViperGPT: Visual Inference via Python Execution for Reasoning"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2303.08128)] <br />
    Dídac Surís, Sachit Menon, Carl Vondrick

12. "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2304.10592)] <br />
    Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny
    
13. "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2305.06500)] <br />
    Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi

14. "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
    ICML (2023).
    [[paper](https://arxiv.org/abs/2301.12597)] <br />
    Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi
    
15. “VideoChat: Chat-Centric Video Understanding”
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2305.06355)] <br />
    KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao
    
16. "Self-Chained Image-Language Model for Video Localization and Question Answering"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2305.06988)] <br />
    Shoubin Yu, Jaemin Cho, Prateek Yadav, Mohit Bansal




## LLM in Data Generation 

1. "Generate labeled training data using Prompt Programming and GPT-3. An example of Big Five Personality Classification"
    Arxiv (2023).
    [[paper](https://arxiv.org/ftp/arxiv/papers/2303/2303.12279.pdf)] <br />
    Eason Chen
    
2. "AugGPT: Leveraging ChatGPT for Text Data Augmentation"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2302.13007)] <br />
    Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun, Quanzheng Li, Dinggang Shen, Tianming Liu, Xiang Li

3. "Reward Design with Language Models"
    ICLR (2023).
    [[paper](https://arxiv.org/pdf/2303.00001.pdf)] <br />
    Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh

4. "Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2304.13861)] <br />
    Anders Giovanni Møller, Jacob Aarup Dalsgaard, Arianna Pera, Luca Maria Aiello

## Hallucination in LLM

1. "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models"
    Arxiv (2023).
    [[paper](https://arxiv.org/pdf/2303.08896.pdf)] <br />
    Potsawee Manakul, Adian Liusie, Mark J. F. Gales

2. "Reflexion: an autonomous agent with dynamic memory and self-reflection"
    Arxiv (2023).
    [[paper](https://arxiv.org/pdf/2303.11366.pdf)] <br />
    Noah Shinn, Beck Labash, Ashwin Gopinath
    
## In-Context Learning

1. "Fairness-guided Few-shot Prompting for Large Language Models"
    Arxiv (2023).
    [[paper](https://arxiv.org/pdf/2303.13217.pdf)] <br />
    Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, Bingzhe Wu

2. "Larger language models do in-context learning differently"
    Arxiv (2023).
    [[paper](https://arxiv.org/pdf/2303.03846.pdf)] <br />
    Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, Tengyu Ma
    
3. "Automatic Chain of Thought Prompting in Large Language Models"
    ICLR (2023).
    [[paper](https://arxiv.org/pdf/2210.03493.pdf)] <br />
    Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola
    
