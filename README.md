# LLM-research-paper
paper list about large language model (LLM)

## LLM Evaluation

1. "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2302.06476)] <br />
    Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang
  
2. "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2302.04023)] <br />
    Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, Pascale Fung
    
## Efficient Tuning

1. "VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"
    CVPR (2022).
    [[paper](https://arxiv.org/abs/2112.06825)] <br />
    Yi-Lin Sung, Jaemin Cho, Mohit Bansal

2. "Multimodal Few-Shot Learning with Frozen Language Models"
    NIPS (2021).
    [[paper](https://proceedings.neurips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf)] <br />
    Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, Felix Hill
    

3. "Modular and Parameter-Efficient Multimodal Fusion with Prompting"
    Findings of ACL (2022).
    [[paper](https://aclanthology.org/2022.findings-acl.234/)] <br />
    Sheng Liang, Mengjie Zhao, Hinrich Schuetze

4. "Learning to prompt for vision-language models"
    IJCV (2022).
    [[paper](https://arxiv.org/pdf/2109.01134.pdf)] <br />
    Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu 
    
5. "Conditional prompt learning for vision-language models"
    CVPR (2022).
    [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf)] <br />
    Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu
    
6. "Maple: Multi-modal prompt learning"
    Arxiv (2022).
    [[paper](https://arxiv.org/pdf/2210.03117.pdf)] <br />
    Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan

## LLM with Multimodal 

1. "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning"
    Arxiv (2023).
    [[paper](https://arxiv.org/pdf/2301.05226.pdf)] <br />
    Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, Chuang Gan
    
2. "Multimodal Chain-of-Thought Reasoning in Language Models"
    Arxiv (2023).
    [[paper](https://arxiv.org/abs/2302.00923)] <br />
    Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola
    
3. "Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering"
    CVPR (2023).
    [[paper](https://arxiv.org/abs/2303.01903)]
    [[code]([https://github.com/ZrrSkywalker/CaFo](https://github.com/MILVLG/prophet))] <br />
    Zhenwei Shao, Zhou Yu, Meng Wang, Jun Yu
    
4. "Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners"
    CVPR (2023).
    [[paper](https://arxiv.org/abs/2303.02151)]
    [[code](https://github.com/ZrrSkywalker/CaFo)] <br />
    Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, Peng Gao
    
5. "An empirical study of gpt-3 for few-shot knowledge-based vqa"
    AAAI (2022).
    [[paper](https://arxiv.org/abs/2109.05014)] <br />
    Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, Lijuan Wang
    
6. "KAT: A Knowledge Augmented Transformer for Vision-and-Language"
    NAACL (2022).
    [[paper](https://aclanthology.org/2022.naacl-main.70/)] <br />
    Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, Jianfeng Gao
    
7. "REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering"
    NIPS (2022).
    [[paper](https://arxiv.org/abs/2206.01201)] <br />
    Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chenguang Zhu, Lu Yuan
    
8. "DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training"
    ICLR (2023).
    [[paper](https://arxiv.org/abs/2303.03032)] <br />
    Wei Li, Linchao Zhu, Longyin Wen, Yi Yang

9. 


